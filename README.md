Core question: Can a small LLM be fine-tuned to express uncertainty rather than hallucinate answers, while still providing useful reasoning when confident?



###  This project was developed and tested on Google Colab (Free tier).

- Python: 3.12.12
- GPU: NVIDIA T4 (16 GB)
- CUDA: Provided and managed by Colab
- Framework: PyTorch + Hugging Face ecosystem
