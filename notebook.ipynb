{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UdrkJ1daG7Vn",
        "outputId": "5238f1a2-e3f4-46c0-9a21-6a00bcab7b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UdrkJ1daG7Vn",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/uncertainty-aware-llms')"
      ],
      "metadata": {
        "id": "5iDHDNV2G8aP"
      },
      "id": "5iDHDNV2G8aP",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl"
      ],
      "metadata": {
        "id": "vnrQBO9eH-1z",
        "outputId": "025bee56-d91f-4c19-87de-a28bff8299ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vnrQBO9eH-1z",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages for finetuning\n"
      ],
      "metadata": {
        "id": "jN3M4soOIU_4"
      },
      "id": "jN3M4soOIU_4"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import(\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig,PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "k3FkW0F6IRpt"
      },
      "id": "k3FkW0F6IRpt",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Login"
      ],
      "metadata": {
        "id": "8doLDOT0KC0b"
      },
      "id": "8doLDOT0KC0b"
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "na6JTjcjKCnr",
        "outputId": "79c39105-4982-414e-d1d7-4909bd7f0dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "na6JTjcjKCnr",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Uncertainity` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Uncertainity`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the base model and dataset"
      ],
      "metadata": {
        "id": "_fF6lupnJehU"
      },
      "id": "_fF6lupnJehU"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "datset_name = \"suyog-ghimire/UncertaintyQA\"\n",
        "new_model = \"llama-3.2-1B-Uncertainity\""
      ],
      "metadata": {
        "id": "ANgEMnmEJeRt"
      },
      "id": "ANgEMnmEJeRt",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QLoRA parameters"
      ],
      "metadata": {
        "id": "ZAGdhkbpK07T"
      },
      "id": "ZAGdhkbpK07T"
    },
    {
      "cell_type": "code",
      "source": [
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1"
      ],
      "metadata": {
        "id": "AUiY5ppCI6st"
      },
      "id": "AUiY5ppCI6st",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bits and Bytes parameters"
      ],
      "metadata": {
        "id": "w3tM0MbhLAlZ"
      },
      "id": "w3tM0MbhLAlZ"
    },
    {
      "cell_type": "code",
      "source": [
        "use_4bit = True # Activate 4-bit precision base model loading\n",
        "bnb_4bit_compute_dtype = \"float16\"  # computations are done in F16\n",
        "bnb_4bit_quant_type = \"nf4\" # Choosing Quantization type (fp4 or nf4)\n",
        "use_nested_quant = False"
      ],
      "metadata": {
        "id": "OenUXPr9K_nQ"
      },
      "id": "OenUXPr9K_nQ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovnvh0aALH5R"
      },
      "id": "ovnvh0aALH5R",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}